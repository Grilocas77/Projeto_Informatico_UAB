# ============================================================
# chatbot_engine.py - Duarte Grilo 2201320 - Projeto Inform√°tico
# ============================================================
# Objetivo:
# üîπ Carregar modelo de embeddings (SentenceTransformer) para busca sem√¢ntica
# üîπ Aceder √† base ChromaDB com documentos embebidos
# üîπ Realizar busca heur√≠stica com similaridade vetorial
# üîπ Identificar o melhor documento com base em similaridade de cosseno
# üîπ Suporte para retorno direto ou impress√£o da melhor resposta
# ============================================================

from sentence_transformers import SentenceTransformer, util
from models.rag_engine import get_chroma_db
from config import EMBEDDING_MODEL, TOP_K_SEARCH, MAX_DOC_CHARS
from controllers.logger import log_evento


def busca_local_heuristica(
    pergunta: str,
    k: int = TOP_K_SEARCH,
    retorna: bool = False
) -> str:
    """
    Realiza busca sem√¢ntica local com embeddings e ChromaDB.
    Args:
        pergunta (str): Pergunta do utilizador (em portugu√™s ou ingl√™s).
        k (int): N√∫mero de documentos a recuperar (_top-k_).
        retorna (bool): Se True, retorna _string_ da melhor resposta, sen√£o imprime.
    Returns:
        str: Melhor resposta encontrada ou mensagem padr√£o.
    Observa√ß√µes:
        - Suporta lista de dicts ou lista de tuplas (dict, ...).
        - Limita resposta a MAX_DOC_CHARS carateres.
        - Todos os eventos relevantes s√£o registados via logger.
    """
    log_evento(f"üîç Pergunta recebida: {pergunta}")

    # Carrega modelo de embeddings definido em config
    model = SentenceTransformer(EMBEDDING_MODEL)

    # Acede √† base local de embeddings via ChromaDB
    collection = get_chroma_db()
    resultados = collection.similarity_search(query=pergunta, k=k)

    if not resultados:
        log_evento("Nenhum resultado encontrado na busca sem√¢ntica.")
        return "Desculpe, n√£o encontrei informa√ß√µes relevantes."

    # Normaliza resultados (lista de dicts ou lista de tuplas)
    if isinstance(resultados[0], tuple):
        melhores_docs = [d[0]['page_content'] for d in resultados if isinstance(d, tuple) and len(d) > 0 and 'page_content' in d[0]]
    else:
        melhores_docs = [d['page_content'] for d in resultados if 'page_content' in d]

    if not melhores_docs:
        log_evento("Resultados encontrados mas sem texto v√°lido em 'page_content'.")
        return "Desculpe, n√£o encontrei informa√ß√µes relevantes."

    # Encontra o melhor documento por similaridade de cosseno
    emb_pergunta = model.encode(pergunta, convert_to_tensor=True)
    melhor_doc = max(
        melhores_docs,
        key=lambda d: util.pytorch_cos_sim(emb_pergunta, model.encode([d], convert_to_tensor=True)).mean().item()
    )

    melhor_doc = melhor_doc[:MAX_DOC_CHARS]
    log_evento(f"Melhor resposta selecionada (primeiros 100 chars): {melhor_doc[:100]}...")

    if retorna:
        return melhor_doc

    print(f"\n‚úÖ Melhor resposta:\n{melhor_doc}")
