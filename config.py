# ============================================================
# config.py - Duarte Grilo 2201320 - Projeto Inform치tico
# ============================================================
# Objetivo:
# Definir as configura칞칫es centrais do sistema, incluindo:
#
# 游댳 Diret칩rios principais (documentos, embeddings)
# 游댳 Modelos utilizados (embeddings, reranker, LLM, tradu칞칚o)
# 游댳 _Tokens e par칙metros para APIs externas (HuggingFace, Discord)
# 游댳 Par칙metros para gera칞칚o de respostas e processamento de texto
# 游댳 _Templates e constantes usados em diversos m칩dulos
# ============================================================

import os
from dotenv import load_dotenv
from transformers import MarianMTModel, MarianTokenizer

# 游댳 Carrega vari치veis de ambiente a partir do ficheiro .env (ex: tokens API)
load_dotenv()

# 游댳 Diret칩rios principais do projeto
DOCUMENTS_PATH = "data/documents"          # Onde ficam os ficheiros a indexar
CHROMA_PATH = "embeddings_db/chroma_db"    # Caminho para a base de embeddings ChromaDB

# 游댳 Modelos a utilizar
EMBEDDING_MODEL = "intfloat/e5-large-v2"   # Modelo de embeddings sem칙nticos
RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"  # Modelo para reranking
VELVET_MODEL = "Almawave/Velvet-2B"        # Modelo de linguagem principal (LLM)

# 游댳 Token da Hugging Face (para acesso aos modelos online)
HF_TOKEN = os.getenv("HF_TOKEN")

# 游댳 Tradutores pr칠-carregados com MarianMT (usados no m칩dulo tradutor_local.py)
TRADUTOR_MODELOS = {
    "pt-en": {
        "tokenizer": MarianTokenizer.from_pretrained("geralt/Opus-mt-pt-en"),
        "model": MarianMTModel.from_pretrained("geralt/Opus-mt-pt-en")
    },
    "en-pt": {
        "tokenizer": MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-tc-big-en-pt"),
        "model": MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-tc-big-en-pt")
    }
}

# 游댳 Token do bot do Discord (se usado)
DISCORD_BOT_TOKEN = os.getenv("DISCORD_BOT_TOKEN")

# 游댳 Par칙metros de gera칞칚o do modelo Velvet
VELVET_PARAMS = {
    "max_new_tokens": 300,          # N칰mero m치ximo de tokens gerados
    "do_sample": True,              # Ativa amostragem
    "temperature": 0.3,             # Temperatura baixa = respostas mais determin칤sticas
    "top_p": 0.8,                   # Nucleus sampling
    "repetition_penalty": 1.2       # Penaliza repeti칞칫es
}

# 游댳 _Template para gera칞칚o de prompts no modo CLI/GUI
CHATBOT_PROMPT_TEMPLATE = (
    "Contexto:\n{contexto}\n\n"
    "Pergunta: {user_input}\n"
    "Resposta:"
)

# 游댳 Par칙metros usados no split e busca de documentos (RAG)
CHUNK_SIZE = 1200             # Tamanho de cada chunk (bloco de texto)
CHUNK_OVERLAP = 200          # Sobreposi칞칚o entre chunks para n칚o perder contexto
K_SIMILARITY_SEARCH = 7      # N칰mero de documentos mais semelhantes a retornar

# Par칙metro para _top-k documentos retornados nas buscas sem칙nticas
TOP_K_SEARCH = 5

# Limite de caracteres da resposta que 칠 apresentada ao utilizador
MAX_DOC_CHARS = 800

MAX_PROMPT_TOKENS = 512
