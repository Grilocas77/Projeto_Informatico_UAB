# ============================================================
# velvet_runner.py - Duarte Grilo 2201320 - Projeto Inform√°tico
# ============================================================
# Este m√≥dulo √© respons√°vel por:
# üîπ Carregar o modelo de linguagem Velvet (HF)
# üîπ Gerar respostas traduzidas (PT ‚ûú EN ‚ûú PT)
# üîπ Validar respostas por palavras-chave
# üîπ Guardar as respostas geradas:
#     - Num ficheiro de hist√≥rico (`velvet_respostas.json`)
#     - Num ficheiro tempor√°rio com a √∫ltima resposta (`velvet_ultima_resposta.json`)
# ============================================================

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from config import VELVET_MODEL, HF_TOKEN, VELVET_PARAMS
import json
import os
from pathlib import Path
from models.tradutor_local import traduzir_pt_para_en, traduzir_en_para_pt

# ============================================================
# üîß Carregamento do modelo Velvet-2B a partir do HuggingFace
# ============================================================


def load_model():
    # Tokenizer e modelo definidos no config.py
    tokenizer = AutoTokenizer.from_pretrained(VELVET_MODEL, token=HF_TOKEN)
    model = AutoModelForCausalLM.from_pretrained(VELVET_MODEL, token=HF_TOKEN)

    # Cria um pipeline de gera√ß√£o de texto com os par√¢metros definidos
    return pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=0,  # Usa GPU (0) se dispon√≠vel; CPU (-1) se n√£o
        **VELVET_PARAMS  # Ex: max_new_tokens, temperature, top_k, etc.
    )


# Inicializa√ß√£o do pipeline ao carregar o m√≥dulo
model_generator = load_model()

# ============================================================
# üß† Gera√ß√£o de resposta traduzida com Velvet
# ============================================================


def generate_response(prompt: str) -> str:
    # Traduz o prompt do utilizador para ingl√™s
    prompt_en = traduzir_pt_para_en(prompt)

    # Gera resposta com Velvet
    generated = model_generator(prompt_en)

    # Extrai o texto gerado
    resposta_en = generated[0]['generated_text']

    # Traduz de volta para portugu√™s
    resposta_pt = traduzir_en_para_pt(resposta_en)

    # Remove prefixos como "Resposta:" caso existam
    resposta_limpa = resposta_pt.split("Resposta:")[-1].strip()
    return resposta_limpa

# ============================================================
# ‚úÖ Valida√ß√£o de resposta com base em palavras-chave
# ============================================================


def validar_resposta_por_keywords(resposta: str, contexto: str, min_match: int = 3) -> bool:
    """
    Verifica se a resposta cont√©m pelo menos `min_match` palavras com 4+ letras
    em comum com o contexto fornecido.
    """
    import re
    palavras_contexto = re.findall(r'\b\w{4,}\b', contexto.lower())
    palavras_resposta = re.findall(r'\b\w{4,}\b', resposta.lower())
    comuns = set(palavras_contexto).intersection(set(palavras_resposta))
    return len(comuns) >= min_match

# ============================================================
# üíæ Gest√£o de ficheiros: hist√≥rico e √∫ltima resposta
# ============================================================


# Caminhos dos ficheiros JSON
VELVET_HIST_FILE = Path("data/respostas/velvet_respostas.json")
VELVET_LAST_FILE = Path("data/respostas/velvet_ultima_resposta.json")

# Garante que a pasta 'data/respostas' existe
os.makedirs(VELVET_HIST_FILE.parent, exist_ok=True)


def salvar_completo_em_arquivo(detalhes: dict):
    """
    Guarda os detalhes completos da intera√ß√£o com o modelo.
    Cria dois ficheiros:
    1. Hist√≥rico acumulado (JSON com lista)
    2. √öltima resposta (JSON individual)
    """

    # 1. Carrega ou inicia o hist√≥rico
    if VELVET_HIST_FILE.exists():
        with open(VELVET_HIST_FILE, encoding="utf-8") as f:
            historico = json.load(f)
    else:
        historico = []

    # 2. Acrescenta a nova entrada ao hist√≥rico
    historico.append(detalhes)

    # 3. Guarda o hist√≥rico atualizado
    with open(VELVET_HIST_FILE, "w", encoding="utf-8") as f:
        json.dump(historico, f, indent=2, ensure_ascii=False)

    # 4. Atualiza o ficheiro com a √∫ltima resposta
    with open(VELVET_LAST_FILE, "w", encoding="utf-8") as f:
        json.dump(detalhes, f, indent=2, ensure_ascii=False)

    # 5. Confirma√ß√£o no terminal
    print("üìù Resposta guardada em:")
    print(f"   ‚îî Hist√≥rico ‚Üí {VELVET_HIST_FILE}")
    print(f"   ‚îî √öltima resposta ‚Üí {VELVET_LAST_FILE}")
